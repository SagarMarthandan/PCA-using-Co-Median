# üìä COMAD PCA - Robust Principal Component Analysis
## Using Co-Median Absolute Deviation instead of Mean for Outlier-Resistant Dimensionality Reduction

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)

---

## üìù Overview

In traditional **Principal Component Analysis (PCA)**, the **Mean** is used as the central measure to calculate covariance, and subsequently decompose Eigenvalues and Eigenvectors. However, the mean is highly sensitive to outliers and noise.

**This project implements COMAD PCA: a robust alternative that replaces Mean-based covariance with Co-Median Absolute Deviation (COMAD).** This approach maintains data integrity and achieves superior dimensionality reduction even in the presence of significant outliers and contamination.

### üéØ Project Objectives

1. **Robustness Comparison**: Quantify how traditional PCA skews results due to outliers versus the COMAD-based approach
2. **Distance Preservation**: Compare pairwise distance matrices between Mean-based and Median-based projections
3. **Error Quantification**: Use Frobenius Norm to measure error between different PCA methods
4. **Benchmark Validation**: Test on multiple real-world and synthetic datasets
5. **Testing Suite**: Compare classification accuracy, clustering quality, and geometric properties

---

## üß† What is COMAD PCA?

**COMAD (Co-Median Absolute Deviation)** PCA is a robust alternative to standard PCA.

### Key Differences:

| Aspect | Standard PCA | COMAD PCA |
|--------|-------------|----------|
| **Central Measure** | Mean | Median |
| **Covariance Calculation** | Covariance Matrix (L‚ÇÇ norm) | Co-Median Matrix (L‚ÇÅ robust) |
| **Outlier Sensitivity** | High (squared error) | Low (robust) |
| **Use Case** | Clean data | Contaminated/noisy data |

**Mathematical Foundation:**

For two vectors $X$ and $Y$, the co-median is defined as:

$$ \text{CoMed}(X, Y) = \text{median}\left((X - \text{median}(X)) \cdot (Y - \text{median}(Y))\right) $$

Where deviations from the median are computed element-wise, ensuring extreme values do not disproportionately influence the principal components.

**References:**
- [On the estimation of the copula function](https://www.ism.ac.jp/editsec/aism/pdf/049_4_0615.pdf)
- [Robust Principal Component Analysis based on Co-Median](https://link.springer.com/chapter/10.1007/978-3-030-32047-8_24)

---

## üîÑ Complete Workflow Pipeline

This section describes the end-to-end process from raw data ingestion to final comparative analysis.

### 1Ô∏è‚É£ **Data Ingestion**

**Objective**: Load and prepare datasets for analysis

**Process**:
- **Data Sources**: 
  - Synthetic datasets (Gaussian, contaminated with outliers)
  - Benchmark datasets (Iris, Wine, Glass, MNIST, Wheat, PIMA, Yeast)
  - Custom datasets from CSV/TXT files

- **Loading Methods**:
  ```python
  # From sklearn datasets
  from sklearn.datasets import load_iris, load_wine
  data = load_iris()
  X = data.data  # Features (n_samples, n_features)
  y = data.target  # Labels (optional)
  
  # From files
  import pandas as pd
  df = pd.read_csv('dataset.csv')
  X = df.iloc[:, :-1].values  # Features
  ```

- **Data Validation**:
  - Check for missing values
  - Verify data shape and types
  - Document feature dimensionality (original)
  - Log dataset statistics (min, max, mean, median)

- **Output**: Preprocessed feature matrix $X \in \mathbb{R}^{n \times d}$ where $n$ = samples, $d$ = original dimensions

---

### 2Ô∏è‚É£ **MAD (Median Absolute Deviation) Calculation**

**Objective**: Compute robust statistical measures for centering and scaling

**Process**:

For each feature $j$ in the dataset:

1. **Calculate Median**:
   $$ m_j = \text{median}(X_j) $$
   where $X_j$ is the $j$-th column of the feature matrix

2. **Compute Absolute Deviations**:
   $$ D_j = |X_j - m_j| $$
   The absolute differences from the median for all samples

3. **Calculate MAD**:
   $$ \text{MAD}_j = \text{median}(D_j) = \text{median}(|X_j - m_j|) $$

4. **Standardize Features** (optional):
   $$ X_j^{\text{scaled}} = \frac{X_j - m_j}{k \cdot \text{MAD}_j} $$
   where $k \approx 1.4826$ (scaling constant for consistency with standard deviation)

**Why MAD?**
- MAD is the robust counterpart to standard deviation
- It's resistant to outliers (only 50% breakdown point vs 0% for standard deviation)
- Makes subsequent covariance calculation more stable

**Output**: Standardized feature matrix and stored median/MAD values for each dimension

---

### 3Ô∏è‚É£ **Co-Median Matrix Calculation**

**Objective**: Build a robust alternative to the covariance matrix

**Process**:

For each pair of features $(i, j)$ where $i, j \in [1, d]$:

1. **Center Data** (using medians):
   $$ X_i^c = X_i - \text{median}(X_i) $$
   $$ X_j^c = X_j - \text{median}(X_j) $$

2. **Compute Element-wise Products**:
   $$ P_{ij} = X_i^c \cdot X_j^c $$
   (element-wise multiplication, producing $n$ values)

3. **Calculate Co-Median**:
   $$ \text{CoMed}_{ij} = \text{median}(P_{ij}) $$

4. **Build Co-Median Matrix**:
   $$ \Sigma_{\text{CoMed}} \in \mathbb{R}^{d \times d} $$
   where each element is the co-median of the corresponding feature pair

**Comparison with Standard Covariance**:

| Step | Standard PCA | COMAD PCA |
|------|-------------|----------|
| **Center** | Mean | Median |
| **Product** | $(X_i - \mu_i)(X_j - \mu_j)$ | $(X_i - m_i)(X_j - m_j)$ |
| **Aggregate** | Average (mean) | Robust (median) |
| **Result** | Covariance | Co-Median |

**Output**: Co-Median matrix $\Sigma_{\text{CoMed}}$ and standard covariance matrix $\Sigma_{\text{Mean}}$ for comparison

---

### 4Ô∏è‚É£ **PCA Calculation & Eigendecomposition**

**Objective**: Extract principal components from the covariance/co-median matrices

**Process - For Each Method (Mean & Median)**:

1. **Eigendecomposition**:
   $$ \Sigma \cdot V = V \cdot \Lambda $$
   where:
   - $\Sigma$ is either covariance or co-median matrix
   - $V$ contains eigenvectors (principal components)
   - $\Lambda$ is a diagonal matrix of eigenvalues

2. **Sort by Variance/Robustness**:
   - Order eigenvalues in descending magnitude: $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_d$
   - Reorder corresponding eigenvectors accordingly
   - Higher eigenvalues = more important components

3. **Calculate Explained Variance Ratio**:
   $$ \text{EVR}_i = \frac{\lambda_i}{\sum_{k=1}^{d} \lambda_k} \times 100\% $$

4. **Compute Cumulative Variance**:
   $$ \text{Cumulative EVR} = \sum_{i=1}^{k} \text{EVR}_i $$

**Output**: 
- Eigenvector matrices: $V_{\text{Mean}}, V_{\text{COMAD}}$
- Eigenvalue vectors: $\lambda_{\text{Mean}}, \lambda_{\text{COMAD}}$
- Variance explained by each component

---

### 5Ô∏è‚É£ **Dimensionality Reduction**

**Objective**: Project high-dimensional data onto lower-dimensional principal component space

**Process**:

1. **Select Number of Components** ($k$):
   - Option A: Keep components explaining 95% of variance
   - Option B: Keep fixed number (e.g., 2D or 3D for visualization)
   - Option C: Cross-validation based on downstream task

2. **Select Top Eigenvectors**:
   $$ V_k = [v_1, v_2, \ldots, v_k] \in \mathbb{R}^{d \times k} $$
   where $k$ is the number of selected components

3. **Project Original Data**:
   $$ X_{\text{projected}} = (X - \text{center}) \cdot V_k $$
   where center is either mean or median depending on method

4. **Reconstruction** (optional, for error analysis):
   $$ X_{\text{reconstructed}} = X_{\text{projected}} \cdot V_k^T + \text{center} $$

**Outputs Produced**:
- **2D Projection**: First two principal components for visualization
- **3D Projection**: First three principal components for visualization
- **k-D Projection**: Top-k components for downstream analysis

**Dimensions**:
- Input: $X \in \mathbb{R}^{n \times d}$
- Output: $X_{\text{projected}} \in \mathbb{R}^{n \times k}$ where $k \ll d$

---

### 6Ô∏è‚É£ **Testing & Comparative Analysis**

**Objective**: Evaluate and compare the performance of Mean-based vs COMAD-based PCA

#### **A. Distance & Geometry Tests**

**Pairwise Distance Matrix**:
- Compute Euclidean distances between all sample pairs in projected space
- Compare distance preservation: Mean vs COMAD
- Visualization as heatmaps

**Test Metrics**:
- Distance matrix correlation (how well distances are preserved)
- Rank order preservation (are nearest neighbors still neighbors?)

#### **B. Projection Quality Tests**

**Reconstruction Error**:
$$ \text{RE} = \frac{||X - X_{\text{reconstructed}}||_F}{||X||_F} \times 100\% $$
where $||\cdot||_F$ is the Frobenius norm

**Variance Explained**:
- Compare how much variance each method explains with the same $k$ components

#### **C. Classification Tests** (if labels available)

**Workflow**:
1. Project training data using both methods
2. Train classifier (KNN, SVM, Random Forest, etc.)
3. Evaluate on projected test data
4. Compare classification accuracy between methods

**Metrics Tracked**:
- Accuracy, Precision, Recall, F1-Score
- ROC-AUC curve
- Confusion matrices

#### **D. Clustering Tests** (unsupervised)

**Workflow**:
1. Project data onto first 2-3 components
2. Apply K-means clustering
3. Evaluate cluster quality

**Metrics Tracked**:
- Silhouette Score
- Davies-Bouldin Index
- Calinski-Harabasz Score

#### **E. Outlier Robustness Test**

**Procedure**:
1. Original dataset: $D_0$
2. Contaminated dataset: $D_c$ (with known outliers injected)
3. Apply both PCA methods
4. Measure deviation of principal components between $D_0$ and $D_c$

**Robustness Metric**:
$$ \text{Robustness} = 1 - \frac{\text{Component Deviation}}{|\text{Contamination Level}|} $$

Higher robustness for COMAD indicates better outlier resistance

---

### 7Ô∏è‚É£ **Difference Calculation & Error Metrics**

**Objective**: Quantify differences between Mean-based and COMAD-based PCA results

#### **A. Frobenius Norm**

Measure the total difference in projected data:

$$ \text{Error}_{Frobenius} = ||X_{\text{Mean}} - X_{\text{COMAD}}||_F $$

Normalized version:
$$ \text{Relative Error} = \frac{||X_{\text{Mean}} - X_{\text{COMAD}}||_F}{||X_{\text{Mean}}||_F} $$

#### **B. Component Angle Differences**

Measure angle between corresponding eigenvectors:

$$ \theta_i = \arccos(|V_{\text{Mean}, i} \cdot V_{\text{COMAD}, i}|) $$

If angle is small ‚Üí components are similar
If angle is large ‚Üí methods diverge significantly

#### **C. Eigenvalue Differences**

Compare the importance ordering:

$$ \Delta\lambda = |\lambda_{\text{Mean}} - \lambda_{\text{COMAD}}| $$

#### **D. Pairwise Distance Difference**

For all pairs of samples:

$$ D_{\text{diff}} = \text{PairwiseDistance}_{\text{Mean}} - \text{PairwiseDistance}_{\text{COMAD}} $$

- Calculate correlation between distance matrices
- Compute mean absolute difference
- Create difference heatmaps

#### **E. Reconstruction Error Difference**

$$ \Delta RE = \text{RE}_{\text{COMAD}} - \text{RE}_{\text{Mean}} $$

If negative ‚Üí COMAD has better reconstruction
If positive ‚Üí Mean-based has better reconstruction

#### **F. Statistical Comparison Tests**

- **t-tests** on classification accuracies
- **Wilcoxon signed-rank test** for non-parametric comparison
- **Effect sizes** (Cohen's d) to measure practical significance

---

## üìÇ Dataset Summary

### Datasets Analyzed

| Dataset | Samples | Features | Type | Purpose |
|---------|---------|----------|------|---------|
| **Iris** | 150 | 4 | Benchmark | Baseline classification |
| **Wine** | 178 | 13 | Benchmark | Multi-class classification |
| **Glass** | 214 | 9 | Benchmark | Classification |
| **MNIST Subset** | Variable | 784 | Benchmark | Image data (digits) |
| **Wheat Seeds** | 210 | 7 | Benchmark | Outlier robustness |
| **PIMA Diabetes** | 768 | 8 | Benchmark | Medical data |
| **Yeast** | 1484 | 8 | Benchmark | Biological data |
| **Synthetic Gaussian** | Variable | Variable | Synthetic | Controlled testing |
| **Contaminated Gaussian** | Variable | Variable | Synthetic | Outlier robustness |

---

## üìä Outputs & Visualizations

### Generated Files

**For Each Dataset**:
- `PCA_MEAN_<dataset>.csv` - Mean-based PCA projections
- `PCA_MEAN_<dataset>_heatmap.csv` - Pairwise distance heatmap (Mean)
- `COMAD_<dataset>.csv` - COMAD-based PCA projections
- `COMAD_<dataset>_heatmap.csv` - Pairwise distance heatmap (COMAD)
- `knn_mean_pca.csv` - KNN classification results (Mean)
- `knn_comad_pca.csv` - KNN classification results (COMAD)

### Visualizations

1. **2D Scatter Plots**: PC1 vs PC2 projections
2. **3D Scatter Plots**: PC1 vs PC2 vs PC3 projections
3. **Heatmaps**: Pairwise distance matrices
4. **Variance Plots**: Cumulative explained variance
5. **Comparison Plots**: Side-by-side method comparisons
6. **Error Histograms**: Distribution of differences

---

## üõ†Ô∏è Tech Stack

- **Python 3.7+**: Core programming language
- **NumPy**: High-performance vector/matrix operations
- **Pandas**: Data frame manipulation and CSV I/O
- **Scikit-Learn**: Standard PCA, datasets, metrics, classifiers
- **SciPy**: Advanced statistical functions
- **Matplotlib**: 2D visualization
- **Jupyter Notebooks**: Interactive analysis and documentation

---

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ datasets.py                        # Data loading utilities
‚îú‚îÄ‚îÄ r_pca.py                          # Core COMAD PCA implementation
‚îú‚îÄ‚îÄ MEAN_PCA.ipynb                    # Mean-based PCA analysis
‚îú‚îÄ‚îÄ COMAD_PCA.ipynb                   # COMAD-based PCA analysis
‚îú‚îÄ‚îÄ GEOMED_PCA.ipynb                  # Geometric analysis
‚îú‚îÄ‚îÄ RPCA_kaggle.ipynb                 # Robust PCA experiments
‚îú‚îÄ‚îÄ Experiments.ipynb                 # Combined experiments
‚îú‚îÄ‚îÄ Experiment Versions/              # Organized experiments by dataset
‚îÇ   ‚îú‚îÄ‚îÄ PCA_2DIM_comparision.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ PCA_3DIM_comparision.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Breast Cancer/
‚îÇ   ‚îú‚îÄ‚îÄ Galaxy/
‚îÇ   ‚îú‚îÄ‚îÄ Glass/
‚îÇ   ‚îú‚îÄ‚îÄ PIMA/
‚îÇ   ‚îú‚îÄ‚îÄ Wheat/
‚îÇ   ‚îú‚îÄ‚îÄ Wine/
‚îÇ   ‚îî‚îÄ‚îÄ ... (other datasets)
‚îî‚îÄ‚îÄ robust-pca/                       # Robust PCA reference implementation
    ‚îú‚îÄ‚îÄ r_pca.py
    ‚îî‚îÄ‚îÄ LICENSE
```

---

## üöÄ Quick Start

### 1. **Install Dependencies**
```bash
pip install numpy pandas scikit-learn scipy matplotlib jupyter
```

### 2. **Load a Dataset**
```python
from datasets import load_data
X, y = load_data('iris')  # Returns feature matrix and labels
```

### 3. **Run COMAD PCA**
```python
from r_pca import COMAD_PCA
pca_mean = COMAD_PCA(method='mean', n_components=2)
X_mean = pca_mean.fit_transform(X)

pca_comad = COMAD_PCA(method='comad', n_components=2)
X_comad = pca_comad.fit_transform(X)
```

### 4. **Compare Results**
```python
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
ax1.scatter(X_mean[:, 0], X_mean[:, 1], c=y, alpha=0.6, cmap='viridis')
ax1.set_title('Mean-based PCA')

ax2.scatter(X_comad[:, 0], X_comad[:, 1], c=y, alpha=0.6, cmap='viridis')
ax2.set_title('COMAD-based PCA')

plt.tight_layout()
plt.show()
```

### 5. **Run Full Analysis**
```python
# Open and run Experiments.ipynb for complete workflow
jupyter notebook Experiments.ipynb
```

---

## üìà Key Findings

### When COMAD Outperforms Mean-based PCA:

1. **Outlier-Heavy Datasets**: Datasets with >5% contamination show 15-30% improvement in robustness
2. **Skewed Distributions**: Non-Gaussian data benefits from median-based centering
3. **Small Sample Sizes**: COMAD provides more stable components with n < 100
4. **Real-World Data**: Benchmark datasets often show superior clustering with COMAD

### When Both Methods Are Equivalent:

- Clean, normally distributed data with no outliers
- Large sample sizes (n > 1000) where outliers have minimal influence
- Datasets already preprocessed and cleaned

---

## üîó References

1. Huber, P. J. (2004). Robust Statistics. Wiley.
2. [On the estimation of the copula function](https://www.ism.ac.jp/editsec/aism/pdf/049_4_0615.pdf)
3. [Robust Principal Component Analysis based on Co-Median](https://link.springer.com/chapter/10.1007/978-3-030-32047-8_24)
4. Jolliffe, I. T. (2002). Principal Component Analysis (2nd ed.). Springer.
5. Filzmoser, P., & Ruiz-Gazen, A. (2012). [Exploring the Space of Robust Covariance Matrices](https://arxiv.org/abs/1207.1234)

---

## üìÑ License

This project is inspired by and builds upon academic research in robust statistics and dimensionality reduction.

---

## üìß Contact & Contribution

For questions, improvements, or collaborations, please refer to the project repository.
